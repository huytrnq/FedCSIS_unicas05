{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def load_csv(file_path):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "    # Replace commas in numeric columns and convert them to floats\n",
    "    data = data.replace(',', '.', regex=True).apply(pd.to_numeric, errors='ignore')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_with_excessive_nans(dataframe, threshold=200):\n",
    "    \"\"\"Drop columns from a DataFrame where the number of NaN values exceeds the specified threshold.\"\"\"\n",
    "    nan_counts = dataframe.isna().sum()\n",
    "    columns_to_drop = nan_counts[nan_counts > threshold].index\n",
    "    return dataframe.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_csv('../data/training_data.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_data['Group'] = le.fit_transform(train_data['Group'])\n",
    "\n",
    "# Cut outliers\n",
    "top_quantiles = train_data.quantile(0.97)\n",
    "outliers_top = (train_data > top_quantiles)\n",
    "\n",
    "low_quantiles = train_data.quantile(0.03)\n",
    "outliers_low = (train_data < low_quantiles)\n",
    "\n",
    "train_data = train_data.mask(outliers_top, top_quantiles, axis=1)\n",
    "train_data = train_data.mask(outliers_low, low_quantiles, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.groupby(['Group']).transform(lambda x: x.fillna(x.mean()))\n",
    "train_data = train_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "X = train_data.drop(columns=['Class', 'Perform']).values\n",
    "y = train_data['Class'].values.reshape(-1, 1)\n",
    "\n",
    "# Create an instance of the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform the data\n",
    "one_hot_encoded = encoder.fit_transform(y)\n",
    "\n",
    "# Convert the sparse matrix to a dense array\n",
    "y = one_hot_encoded.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 3)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 3)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(116, 64)  # 116 features\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 3)    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create a model instance\n",
    "model = NeuralNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9743879437446594\n",
      "Epoch 2, Loss: 0.9511961936950684\n",
      "Epoch 3, Loss: 0.9241287112236023\n",
      "Epoch 4, Loss: 0.8854696750640869\n",
      "Epoch 5, Loss: 0.9442274570465088\n",
      "Epoch 6, Loss: 0.9492467641830444\n",
      "Epoch 7, Loss: 0.948595404624939\n",
      "Epoch 8, Loss: 0.9201532006263733\n",
      "Epoch 9, Loss: 0.9517183899879456\n",
      "Epoch 10, Loss: 0.9049546718597412\n",
      "Epoch 11, Loss: 0.8302083611488342\n",
      "Epoch 12, Loss: 0.8976171612739563\n",
      "Epoch 13, Loss: 0.897175133228302\n",
      "Epoch 14, Loss: 0.945582389831543\n",
      "Epoch 15, Loss: 0.9411467909812927\n",
      "Epoch 16, Loss: 0.8641144633293152\n",
      "Epoch 17, Loss: 0.8978013396263123\n",
      "Epoch 18, Loss: 0.9311583638191223\n",
      "Epoch 19, Loss: 0.8450379967689514\n",
      "Epoch 20, Loss: 0.9046963453292847\n",
      "Epoch 21, Loss: 0.8891412019729614\n",
      "Epoch 22, Loss: 0.8917538523674011\n",
      "Epoch 23, Loss: 0.9065621495246887\n",
      "Epoch 24, Loss: 0.8515470623970032\n",
      "Epoch 25, Loss: 0.9030880331993103\n",
      "Epoch 26, Loss: 0.7873167395591736\n",
      "Epoch 27, Loss: 0.8910432457923889\n",
      "Epoch 28, Loss: 0.8413931131362915\n",
      "Epoch 29, Loss: 0.8274245262145996\n",
      "Epoch 30, Loss: 0.7956282496452332\n",
      "Epoch 31, Loss: 0.8487149477005005\n",
      "Epoch 32, Loss: 0.7953395843505859\n",
      "Epoch 33, Loss: 0.872337818145752\n",
      "Epoch 34, Loss: 0.8909364938735962\n",
      "Epoch 35, Loss: 0.869228720664978\n",
      "Epoch 36, Loss: 0.7959812879562378\n",
      "Epoch 37, Loss: 0.8559688925743103\n",
      "Epoch 38, Loss: 0.8276418447494507\n",
      "Epoch 39, Loss: 0.8596305251121521\n",
      "Epoch 40, Loss: 0.866825520992279\n",
      "Epoch 41, Loss: 0.8765206933021545\n",
      "Epoch 42, Loss: 0.9175033569335938\n",
      "Epoch 43, Loss: 0.8282790780067444\n",
      "Epoch 44, Loss: 0.8778232336044312\n",
      "Epoch 45, Loss: 0.6881471872329712\n",
      "Epoch 46, Loss: 0.8776406049728394\n",
      "Epoch 47, Loss: 0.7683895826339722\n",
      "Epoch 48, Loss: 0.907452404499054\n",
      "Epoch 49, Loss: 0.8735337257385254\n",
      "Epoch 50, Loss: 0.8622226715087891\n",
      "Epoch 51, Loss: 0.850222647190094\n",
      "Epoch 52, Loss: 0.78855299949646\n",
      "Epoch 53, Loss: 0.7260154485702515\n",
      "Epoch 54, Loss: 0.966476559638977\n",
      "Epoch 55, Loss: 0.7752766609191895\n",
      "Epoch 56, Loss: 0.870323121547699\n",
      "Epoch 57, Loss: 0.7590977549552917\n",
      "Epoch 58, Loss: 0.8046616315841675\n",
      "Epoch 59, Loss: 0.7317376136779785\n",
      "Epoch 60, Loss: 0.8705235719680786\n",
      "Epoch 61, Loss: 0.784416913986206\n",
      "Epoch 62, Loss: 0.8385264873504639\n",
      "Epoch 63, Loss: 0.9495068192481995\n",
      "Epoch 64, Loss: 0.7580580711364746\n",
      "Epoch 65, Loss: 0.8152582049369812\n",
      "Epoch 66, Loss: 0.7090736627578735\n",
      "Epoch 67, Loss: 0.7743332386016846\n",
      "Epoch 68, Loss: 0.7708876729011536\n",
      "Epoch 69, Loss: 0.8627157807350159\n",
      "Epoch 70, Loss: 0.8446638584136963\n",
      "Epoch 71, Loss: 0.8367862105369568\n",
      "Epoch 72, Loss: 0.8907238245010376\n",
      "Epoch 73, Loss: 0.7746083736419678\n",
      "Epoch 74, Loss: 0.9325276613235474\n",
      "Epoch 75, Loss: 0.8003008365631104\n",
      "Epoch 76, Loss: 0.8133203387260437\n",
      "Epoch 77, Loss: 0.840794563293457\n",
      "Epoch 78, Loss: 0.8887772560119629\n",
      "Epoch 79, Loss: 0.8452771902084351\n",
      "Epoch 80, Loss: 0.8983755707740784\n",
      "Epoch 81, Loss: 0.8506782054901123\n",
      "Epoch 82, Loss: 0.7527226209640503\n",
      "Epoch 83, Loss: 0.8011659383773804\n",
      "Epoch 84, Loss: 0.8146127462387085\n",
      "Epoch 85, Loss: 0.775334894657135\n",
      "Epoch 86, Loss: 0.7326490879058838\n",
      "Epoch 87, Loss: 0.8292174339294434\n",
      "Epoch 88, Loss: 0.784964382648468\n",
      "Epoch 89, Loss: 0.8096879720687866\n",
      "Epoch 90, Loss: 0.7357193231582642\n",
      "Epoch 91, Loss: 0.7760670185089111\n",
      "Epoch 92, Loss: 0.9314596056938171\n",
      "Epoch 93, Loss: 0.8713319301605225\n",
      "Epoch 94, Loss: 0.7534680366516113\n",
      "Epoch 95, Loss: 0.7139301896095276\n",
      "Epoch 96, Loss: 0.7724238038063049\n",
      "Epoch 97, Loss: 0.8219783306121826\n",
      "Epoch 98, Loss: 0.8480551242828369\n",
      "Epoch 99, Loss: 0.8729007244110107\n",
      "Epoch 100, Loss: 0.8397033214569092\n",
      "Epoch 101, Loss: 0.7378780841827393\n",
      "Epoch 102, Loss: 0.7164281010627747\n",
      "Epoch 103, Loss: 0.8160405158996582\n",
      "Epoch 104, Loss: 0.935531735420227\n",
      "Epoch 105, Loss: 0.8741297125816345\n",
      "Epoch 106, Loss: 0.8133257627487183\n",
      "Epoch 107, Loss: 0.8794945478439331\n",
      "Epoch 108, Loss: 0.821239173412323\n",
      "Epoch 109, Loss: 0.8253858089447021\n",
      "Epoch 110, Loss: 0.8266987204551697\n",
      "Epoch 111, Loss: 0.8401877880096436\n",
      "Epoch 112, Loss: 0.8857915997505188\n",
      "Epoch 113, Loss: 0.7053760290145874\n",
      "Epoch 114, Loss: 0.8070319294929504\n",
      "Epoch 115, Loss: 0.750829815864563\n",
      "Epoch 116, Loss: 0.7947500348091125\n",
      "Epoch 117, Loss: 0.7287736535072327\n",
      "Epoch 118, Loss: 0.8800050020217896\n",
      "Epoch 119, Loss: 0.783383309841156\n",
      "Epoch 120, Loss: 0.8980721235275269\n",
      "Epoch 121, Loss: 0.7641326785087585\n",
      "Epoch 122, Loss: 0.8284090161323547\n",
      "Epoch 123, Loss: 0.8170185685157776\n",
      "Epoch 124, Loss: 0.7609832882881165\n",
      "Epoch 125, Loss: 0.7806576490402222\n",
      "Epoch 126, Loss: 0.8938086628913879\n",
      "Epoch 127, Loss: 0.8133516907691956\n",
      "Epoch 128, Loss: 0.7033001184463501\n",
      "Epoch 129, Loss: 0.7487597465515137\n",
      "Epoch 130, Loss: 0.7216152548789978\n",
      "Epoch 131, Loss: 0.7998563647270203\n",
      "Epoch 132, Loss: 0.7288443446159363\n",
      "Epoch 133, Loss: 0.7034767866134644\n",
      "Epoch 134, Loss: 0.7607600688934326\n",
      "Epoch 135, Loss: 0.9108650088310242\n",
      "Epoch 136, Loss: 0.8131589889526367\n",
      "Epoch 137, Loss: 0.7218949198722839\n",
      "Epoch 138, Loss: 0.8799765110015869\n",
      "Epoch 139, Loss: 0.7478402853012085\n",
      "Epoch 140, Loss: 0.7197116613388062\n",
      "Epoch 141, Loss: 0.7536408305168152\n",
      "Epoch 142, Loss: 0.8157328963279724\n",
      "Epoch 143, Loss: 0.7281457781791687\n",
      "Epoch 144, Loss: 0.7245556116104126\n",
      "Epoch 145, Loss: 0.8398643732070923\n",
      "Epoch 146, Loss: 0.7306820154190063\n",
      "Epoch 147, Loss: 0.892560601234436\n",
      "Epoch 148, Loss: 0.7993661165237427\n",
      "Epoch 149, Loss: 0.7921936511993408\n",
      "Epoch 150, Loss: 0.7660369873046875\n",
      "Epoch 151, Loss: 0.7082452178001404\n",
      "Epoch 152, Loss: 0.8399562835693359\n",
      "Epoch 153, Loss: 0.7824827432632446\n",
      "Epoch 154, Loss: 0.8280435800552368\n",
      "Epoch 155, Loss: 0.8192232251167297\n",
      "Epoch 156, Loss: 0.7351108193397522\n",
      "Epoch 157, Loss: 0.7295244336128235\n",
      "Epoch 158, Loss: 0.8044635057449341\n",
      "Epoch 159, Loss: 0.8365567922592163\n",
      "Epoch 160, Loss: 0.8006861805915833\n",
      "Epoch 161, Loss: 0.9166848659515381\n",
      "Epoch 162, Loss: 0.7559878826141357\n",
      "Epoch 163, Loss: 0.7084166407585144\n",
      "Epoch 164, Loss: 0.767368733882904\n",
      "Epoch 165, Loss: 0.8666452765464783\n",
      "Epoch 166, Loss: 0.6935359835624695\n",
      "Epoch 167, Loss: 0.8659529685974121\n",
      "Epoch 168, Loss: 0.916789174079895\n",
      "Epoch 169, Loss: 0.7549697160720825\n",
      "Epoch 170, Loss: 0.8024144172668457\n",
      "Epoch 171, Loss: 0.8573604226112366\n",
      "Epoch 172, Loss: 0.787322998046875\n",
      "Epoch 173, Loss: 0.7015411257743835\n",
      "Epoch 174, Loss: 0.865566611289978\n",
      "Epoch 175, Loss: 0.8490740656852722\n",
      "Epoch 176, Loss: 0.6220534443855286\n",
      "Epoch 177, Loss: 0.6826335787773132\n",
      "Epoch 178, Loss: 0.8433825373649597\n",
      "Epoch 179, Loss: 0.7363935708999634\n",
      "Epoch 180, Loss: 0.8126811385154724\n",
      "Epoch 181, Loss: 0.760297417640686\n",
      "Epoch 182, Loss: 0.6538844108581543\n",
      "Epoch 183, Loss: 0.8009488582611084\n",
      "Epoch 184, Loss: 0.6864456534385681\n",
      "Epoch 185, Loss: 0.7949756979942322\n",
      "Epoch 186, Loss: 0.730864405632019\n",
      "Epoch 187, Loss: 0.9169051647186279\n",
      "Epoch 188, Loss: 0.7254983186721802\n",
      "Epoch 189, Loss: 0.7359961271286011\n",
      "Epoch 190, Loss: 0.7176134586334229\n",
      "Epoch 191, Loss: 0.6778569221496582\n",
      "Epoch 192, Loss: 0.6537138223648071\n",
      "Epoch 193, Loss: 0.7214885354042053\n",
      "Epoch 194, Loss: 0.7256798148155212\n",
      "Epoch 195, Loss: 0.6546788811683655\n",
      "Epoch 196, Loss: 0.8058178424835205\n",
      "Epoch 197, Loss: 0.8931211233139038\n",
      "Epoch 198, Loss: 0.7723520994186401\n",
      "Epoch 199, Loss: 0.7151579856872559\n",
      "Epoch 200, Loss: 0.7663723230361938\n",
      "Epoch 201, Loss: 0.8100491166114807\n",
      "Epoch 202, Loss: 0.7297934293746948\n",
      "Epoch 203, Loss: 0.6311736106872559\n",
      "Epoch 204, Loss: 0.7225263118743896\n",
      "Epoch 205, Loss: 0.770009458065033\n",
      "Epoch 206, Loss: 0.7833162546157837\n",
      "Epoch 207, Loss: 0.6551196575164795\n",
      "Epoch 208, Loss: 0.8052548170089722\n",
      "Epoch 209, Loss: 0.6522717475891113\n",
      "Epoch 210, Loss: 0.7628555297851562\n",
      "Epoch 211, Loss: 0.667585015296936\n",
      "Epoch 212, Loss: 0.6346939206123352\n",
      "Epoch 213, Loss: 0.6680355072021484\n",
      "Epoch 214, Loss: 0.7797224521636963\n",
      "Epoch 215, Loss: 0.72445148229599\n",
      "Epoch 216, Loss: 0.7122501730918884\n",
      "Epoch 217, Loss: 0.707740306854248\n",
      "Epoch 218, Loss: 0.7111261487007141\n",
      "Epoch 219, Loss: 0.8335088491439819\n",
      "Epoch 220, Loss: 0.6817939281463623\n",
      "Epoch 221, Loss: 0.7071430087089539\n",
      "Epoch 222, Loss: 0.7845276594161987\n",
      "Epoch 223, Loss: 0.6339954733848572\n",
      "Epoch 224, Loss: 0.7163680195808411\n",
      "Epoch 225, Loss: 0.6762624979019165\n",
      "Epoch 226, Loss: 0.689030110836029\n",
      "Epoch 227, Loss: 0.7024522423744202\n",
      "Epoch 228, Loss: 0.621256947517395\n",
      "Epoch 229, Loss: 0.7749705910682678\n",
      "Epoch 230, Loss: 0.5788907408714294\n",
      "Epoch 231, Loss: 0.8838176727294922\n",
      "Epoch 232, Loss: 0.7045066952705383\n",
      "Epoch 233, Loss: 0.664010763168335\n",
      "Epoch 234, Loss: 0.7248368263244629\n",
      "Epoch 235, Loss: 0.7292464971542358\n",
      "Epoch 236, Loss: 0.7366412878036499\n",
      "Epoch 237, Loss: 0.7724328637123108\n",
      "Epoch 238, Loss: 0.7511584162712097\n",
      "Epoch 239, Loss: 0.8376129269599915\n",
      "Epoch 240, Loss: 0.7307758331298828\n",
      "Epoch 241, Loss: 0.6085540056228638\n",
      "Epoch 242, Loss: 0.6545851230621338\n",
      "Epoch 243, Loss: 0.5796788334846497\n",
      "Epoch 244, Loss: 0.7601443529129028\n",
      "Epoch 245, Loss: 0.7068161964416504\n",
      "Epoch 246, Loss: 0.7009212970733643\n",
      "Epoch 247, Loss: 0.729600191116333\n",
      "Epoch 248, Loss: 0.6625340580940247\n",
      "Epoch 249, Loss: 0.7617573738098145\n",
      "Epoch 250, Loss: 0.7033941149711609\n",
      "Epoch 251, Loss: 0.7328546047210693\n",
      "Epoch 252, Loss: 0.7088849544525146\n",
      "Epoch 253, Loss: 0.642927885055542\n",
      "Epoch 254, Loss: 0.6331706047058105\n",
      "Epoch 255, Loss: 0.6189643144607544\n",
      "Epoch 256, Loss: 0.7554519772529602\n",
      "Epoch 257, Loss: 0.648982048034668\n",
      "Epoch 258, Loss: 0.6882844567298889\n",
      "Epoch 259, Loss: 0.7787632346153259\n",
      "Epoch 260, Loss: 0.7170495390892029\n",
      "Epoch 261, Loss: 0.6516939401626587\n",
      "Epoch 262, Loss: 0.7402342557907104\n",
      "Epoch 263, Loss: 0.6895791292190552\n",
      "Epoch 264, Loss: 0.7020210027694702\n",
      "Epoch 265, Loss: 0.7310712933540344\n",
      "Epoch 266, Loss: 0.7334394454956055\n",
      "Epoch 267, Loss: 0.8170195817947388\n",
      "Epoch 268, Loss: 0.6896315813064575\n",
      "Epoch 269, Loss: 0.6501791477203369\n",
      "Epoch 270, Loss: 0.7043673396110535\n",
      "Epoch 271, Loss: 0.591494619846344\n",
      "Epoch 272, Loss: 0.6643543839454651\n",
      "Epoch 273, Loss: 0.6778443455696106\n",
      "Epoch 274, Loss: 0.6004292964935303\n",
      "Epoch 275, Loss: 0.724685788154602\n",
      "Epoch 276, Loss: 0.7464666366577148\n",
      "Epoch 277, Loss: 0.6640729904174805\n",
      "Epoch 278, Loss: 0.5194910168647766\n",
      "Epoch 279, Loss: 0.6442737579345703\n",
      "Epoch 280, Loss: 0.7313475608825684\n",
      "Epoch 281, Loss: 0.7644697427749634\n",
      "Epoch 282, Loss: 0.6155809164047241\n",
      "Epoch 283, Loss: 0.6116554737091064\n",
      "Epoch 284, Loss: 0.6549197435379028\n",
      "Epoch 285, Loss: 0.7532507181167603\n",
      "Epoch 286, Loss: 0.748863935470581\n",
      "Epoch 287, Loss: 0.6350514888763428\n",
      "Epoch 288, Loss: 0.733213484287262\n",
      "Epoch 289, Loss: 0.6313002705574036\n",
      "Epoch 290, Loss: 0.756023108959198\n",
      "Epoch 291, Loss: 0.6660881042480469\n",
      "Epoch 292, Loss: 0.7048306465148926\n",
      "Epoch 293, Loss: 0.602070152759552\n",
      "Epoch 294, Loss: 0.6461739540100098\n",
      "Epoch 295, Loss: 0.7096099257469177\n",
      "Epoch 296, Loss: 0.7432270050048828\n",
      "Epoch 297, Loss: 0.7352811098098755\n",
      "Epoch 298, Loss: 0.694419801235199\n",
      "Epoch 299, Loss: 0.7032702565193176\n",
      "Epoch 300, Loss: 0.6969038844108582\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = differentiable_cost_matrix_loss(outputs, labels, cost_matrix)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch + 1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 25.6875%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total = correct = 0\n",
    "\n",
    "y_pred = []\n",
    "y_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.max(labels.data, 1)[1]).sum().item()\n",
    "        y_pred.extend(predicted.numpy().tolist())\n",
    "        y_test.extend(torch.max(labels.data, 1)[1].numpy().tolist())\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8475"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_custom_error(y_test, y_pred, cost_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_csv('../data/test_data_no_target.csv')\n",
    "\n",
    "le = LabelEncoder()\n",
    "test_data['Group'] = le.fit_transform(test_data['Group'])\n",
    "\n",
    "# Cut outliers\n",
    "top_quantiles = test_data.quantile(0.97)\n",
    "outliers_top = (test_data > top_quantiles)\n",
    "\n",
    "low_quantiles = test_data.quantile(0.03)\n",
    "outliers_low = (test_data < low_quantiles)\n",
    "\n",
    "test_data = test_data.mask(outliers_top, top_quantiles, axis=1)\n",
    "test_data = test_data.mask(outliers_low, low_quantiles, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.groupby(['Group']).transform(lambda x: x.fillna(x.mean()))\n",
    "test_data = test_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huytrq/miniconda3/envs/py11/lib/python3.11/site-packages/sklearn/base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_scaled = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "test_tensor = torch.tensor(test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create dataloaders\n",
    "test_dataset = TensorDataset(test_tensor)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total = correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(test_tensor)\n",
    "\n",
    "test_preds = torch.max(outputs, 1)[1].numpy() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('nn.txt', test_preds, fmt='%d', newline='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def differentiable_cost_matrix_loss(outputs, targets, cost_matrix):\n",
    "    \"\"\"\n",
    "    A differentiable loss function based on a cost matrix for one-hot encoded labels.\n",
    "\n",
    "    Args:\n",
    "    outputs (tensor): Raw logits from the neural network.\n",
    "    targets (tensor): Ground truth labels, one-hot encoded.\n",
    "    cost_matrix (tensor): A matrix of costs associated with misclassifications.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The calculated loss.\n",
    "    \"\"\"\n",
    "    # Ensure cost_matrix is a torch tensor and correctly formatted\n",
    "    if not isinstance(cost_matrix, torch.Tensor):\n",
    "        cost_matrix = torch.tensor(cost_matrix, dtype=torch.float32, device=outputs.device)\n",
    "    \n",
    "    # Get softmax probabilities\n",
    "    probs = F.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Compute the effective cost matrix for each prediction\n",
    "    # Matrix multiplication between targets (one-hot encoded) and cost_matrix\n",
    "    # This maps each one-hot vector to its respective cost row in the cost matrix\n",
    "    effective_cost_matrix = torch.matmul(targets, cost_matrix)\n",
    "    \n",
    "    # Calculate the element-wise product of probabilities and the effective cost matrix\n",
    "    weighted_losses = torch.sum(probs * effective_cost_matrix, dim=1)  # sum over classes\n",
    "    \n",
    "    # Mean loss across batch\n",
    "    return torch.mean(weighted_losses)\n",
    "\n",
    "# Example usage\n",
    "cost_matrix = torch.tensor([[0, 1, 2],\n",
    "                            [1, 0, 1],\n",
    "                            [2, 1, 0]], dtype=torch.float32)\n",
    "\n",
    "# Assuming your model, optimizer, and data loaders are set up as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cost_matrix = np.array([[0, 1, 2],\n",
    "                        [1, 0, 1],\n",
    "                        [2, 1, 0]])\n",
    "def calculate_custom_error(preds, gt, cost_matrix=cost_matrix):\n",
    "    \"\"\"\n",
    "    Calculate a custom error metric based on a confusion matrix and a cost matrix.\n",
    "\n",
    "    Args:\n",
    "    preds (array-like): Predicted labels.\n",
    "    gt (array-like): Ground truth (actual) labels.\n",
    "    cost_matrix (numpy.ndarray): A matrix of costs associated with misclassifications.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated error metric.\n",
    "    \"\"\"\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(gt, preds)\n",
    "    \n",
    "    # Validate dimensions of cost_matrix\n",
    "    if cm.shape != cost_matrix.shape:\n",
    "        raise ValueError(\"Cost matrix dimensions must match the confusion matrix dimensions.\")\n",
    "    \n",
    "    # Calculate weighted confusion matrix\n",
    "    weighted_cm = cm * cost_matrix\n",
    "    \n",
    "    # Calculate the custom error\n",
    "    total_samples = len(gt)\n",
    "    if total_samples == 0:\n",
    "        raise ValueError(\"The length of ground truth cannot be zero.\")\n",
    "    \n",
    "    error = np.sum(weighted_cm) / total_samples\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rg/2t9xl9h93wjdygv93x4f6s4r0000gn/T/ipykernel_47927/2990636638.py:2: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  train_data = train_data.replace(',', '.', regex=True).apply(pd.to_numeric, errors='ignore')\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('training_data.csv', delimiter=';')\n",
    "train_data = train_data.replace(',', '.', regex=True).apply(pd.to_numeric, errors='ignore')\n",
    "train_data = train_data[['Perform', 'Class']]\n",
    "\n",
    "test_data = pd.read_csv('test_data_predictions.csv', header=None)\n",
    "\n",
    "X = train_data.drop('Class', axis=1)  # Features\n",
    "y = train_data['Class'] # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(random_state=42)\n",
    "\n",
    "# Initialize search\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.998125\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      1.00      1.00       619\n",
      "           0       1.00      0.99      0.99       227\n",
      "           1       1.00      1.00      1.00       754\n",
      "\n",
      "    accuracy                           1.00      1600\n",
      "   macro avg       1.00      1.00      1.00      1600\n",
      "weighted avg       1.00      1.00      1.00      1600\n",
      "\n",
      "Confusion Matrix:\n",
      "[[619   0   0]\n",
      " [  1 224   2]\n",
      " [  0   0 754]]\n",
      "Custom Error: 0.001875\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Custom Error:\", calculate_custom_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huytrq/miniconda3/envs/py11/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but SVC was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "predicts = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('regression.txt', predicts, fmt='%d', newline='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
