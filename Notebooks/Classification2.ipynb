{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "def load_csv(file_path):\n",
    "    # Load the data\n",
    "    data = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "    # Replace commas in numeric columns and convert them to floats\n",
    "    data = data.replace(',', '.', regex=True).apply(pd.to_numeric, errors='ignore')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_with_excessive_nans(dataframe, threshold=200):\n",
    "    \"\"\"Drop columns from a DataFrame where the number of NaN values exceeds the specified threshold.\"\"\"\n",
    "    nan_counts = dataframe.isna().sum()\n",
    "    columns_to_drop = nan_counts[nan_counts > threshold].index\n",
    "    return dataframe.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cost_matrix = np.array([[0, 1, 2],\n",
    "                        [1, 0, 1],\n",
    "                        [2, 1, 0]])\n",
    "def calculate_custom_error(preds, gt, cost_matrix=cost_matrix):\n",
    "    \"\"\"\n",
    "    Calculate a custom error metric based on a confusion matrix and a cost matrix.\n",
    "\n",
    "    Args:\n",
    "    preds (array-like): Predicted labels.\n",
    "    gt (array-like): Ground truth (actual) labels.\n",
    "    cost_matrix (numpy.ndarray): A matrix of costs associated with misclassifications.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated error metric.\n",
    "    \"\"\"\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(gt, preds)\n",
    "    \n",
    "    # Validate dimensions of cost_matrix\n",
    "    if cm.shape != cost_matrix.shape:\n",
    "        raise ValueError(\"Cost matrix dimensions must match the confusion matrix dimensions.\")\n",
    "    \n",
    "    # Calculate weighted confusion matrix\n",
    "    weighted_cm = cm * cost_matrix\n",
    "    \n",
    "    # Calculate the custom error\n",
    "    total_samples = len(gt)\n",
    "    if total_samples == 0:\n",
    "        raise ValueError(\"The length of ground truth cannot be zero.\")\n",
    "    \n",
    "    error = np.sum(weighted_cm) / total_samples\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_csv('../data/training_data.csv')\n",
    "train_data = train_data[['I5', 'I8', 'I9', 'I18', 'I37', 'I38', 'I44', 'I47', 'I57', 'dI5',\n",
    "       'dI6', 'dI23', 'dI25', 'dI28', 'dI35', 'dI40', 'dI42', 'dI46', 'dI47',\n",
    "       'dI54', 'dI56', 'dI57', 'dI58', 'Group', 'Class', 'Perform']]\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_data['Group'] = le.fit_transform(train_data['Group'])\n",
    "\n",
    "# Cut outliers\n",
    "top_quantiles = train_data.quantile(0.97)\n",
    "outliers_top = (train_data > top_quantiles)\n",
    "\n",
    "low_quantiles = train_data.quantile(0.03)\n",
    "outliers_low = (train_data < low_quantiles)\n",
    "\n",
    "train_data = train_data.mask(outliers_top, top_quantiles, axis=1)\n",
    "train_data = train_data.mask(outliers_low, low_quantiles, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.groupby(['Group']).transform(lambda x: x.fillna(x.mean()))\n",
    "train_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns=['Class', 'Perform'], axis=1)  # Features\n",
    "performs = train_data['Perform']\n",
    "y = train_data['Class'] # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1,  0,  1]), array([2477,  909, 3014]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "np.unique(y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "state = 42\n",
    "\n",
    "model = RandomForestClassifier(random_state=state, n_estimators=100, class_weight='balanced', verbose=1)\n",
    "# model = SVC(random_state=state, class_weight='balanced')\n",
    "# model = KNeighborsClassifier(n_neighbors=3, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Decision Tree Classifier\n",
    "# model = DecisionTreeClassifier(random_state=42)\n",
    "# model = GaussianNB()\n",
    "\n",
    "# Initialize search\n",
    "# model.fit(X_train_res, y_train_res)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.474375\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.44      0.36      0.39       619\n",
      "           0       0.33      0.00      0.01       227\n",
      "           1       0.49      0.71      0.58       754\n",
      "\n",
      "    accuracy                           0.47      1600\n",
      "   macro avg       0.42      0.36      0.33      1600\n",
      "weighted avg       0.45      0.47      0.43      1600\n",
      "\n",
      "Confusion Matrix:\n",
      "[[222   2 395]\n",
      " [ 70   1 156]\n",
      " [218   0 536]]\n",
      "Custom Error: 0.90875\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"Custom Error:\", calculate_custom_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data = load_csv('../data/test_data_no_target.csv')\n",
    "# test_data = drop_columns_with_excessive_nans(test_data, 50)\n",
    "le = LabelEncoder()\n",
    "test_data['Group'] = le.fit_transform(test_data['Group'])\n",
    "\n",
    "# Cut outliers\n",
    "top_quantiles = test_data.quantile(0.97)\n",
    "outliers_top = (test_data > top_quantiles)\n",
    "\n",
    "low_quantiles = test_data.quantile(0.03)\n",
    "outliers_low = (test_data < low_quantiles)\n",
    "\n",
    "test_data = test_data.mask(outliers_top, top_quantiles, axis=1)\n",
    "test_data = test_data.mask(outliers_low, low_quantiles, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.groupby(['Group']).transform(lambda x: x.fillna(x.mean()))\n",
    "test_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.0s\n"
     ]
    }
   ],
   "source": [
    "predicts = model.predict(test_data) + model1.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('predicts_dummy.txt', np.array(predicts), fmt='%d', delimiter='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
