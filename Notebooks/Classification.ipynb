{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load Data\n",
    "training_data = pd.read_csv('../data/training_data.csv', delimiter=';')\n",
    "test_data = pd.read_csv('../data/test_data_no_target.csv', delimiter=';')\n",
    "\n",
    "# Step 2: Convert Numerical Columns from strings to floats\n",
    "def convert_to_float(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' and col not in ['Group', 'Class', 'Perform']:\n",
    "            df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "    return df\n",
    "\n",
    "# Apply conversion to the training data\n",
    "training_data = convert_to_float(training_data)\n",
    "test_data = convert_to_float(test_data)\n",
    "\n",
    "# Step 3: Handle Missing Values (fill with the median)\n",
    "training_data.fillna(training_data.median(numeric_only=True), inplace=True)\n",
    "test_data.fillna(test_data.median(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: One-Hot Encoding for the 'Group' column\n",
    "training_data = pd.get_dummies(training_data, columns=['Group'])\n",
    "test_data = pd.get_dummies(test_data, columns=['Group'])\n",
    "\n",
    "training_data = training_data[['I5', 'I8', 'I9', 'I18', 'I37', 'I38', 'I44', 'I47', 'I57', 'dI5',\n",
    "        'dI6', 'dI23', 'dI25', 'dI28', 'dI35', 'dI40', 'dI42', 'dI46', 'dI47',\n",
    "        'dI54', 'dI56', 'dI57', 'dI58', 'Class', 'Perform']]\n",
    "\n",
    "# Ensure the test set has the same columns as the training set\n",
    "missing_cols = set(training_data.columns) - set(test_data.columns) - {'Class', 'Perform'}\n",
    "for col in missing_cols:\n",
    "    test_data[col] = 0\n",
    "test_data = test_data[training_data.columns.drop(['Class', 'Perform'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cost_matrix = np.array([[0, 1, 2],\n",
    "                        [1, 0, 1],\n",
    "                        [2, 1, 0]])\n",
    "def calculate_custom_error(preds, gt, cost_matrix=cost_matrix):\n",
    "    \"\"\"\n",
    "    Calculate a custom error metric based on a confusion matrix and a cost matrix.\n",
    "\n",
    "    Args:\n",
    "    preds (array-like): Predicted labels.\n",
    "    gt (array-like): Ground truth (actual) labels.\n",
    "    cost_matrix (numpy.ndarray): A matrix of costs associated with misclassifications.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated error metric.\n",
    "    \"\"\"\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(gt, preds)\n",
    "    \n",
    "    # Validate dimensions of cost_matrix\n",
    "    if cm.shape != cost_matrix.shape:\n",
    "        raise ValueError(\"Cost matrix dimensions must match the confusion matrix dimensions.\")\n",
    "    \n",
    "    # Calculate weighted confusion matrix\n",
    "    weighted_cm = cm * cost_matrix\n",
    "    \n",
    "    # Calculate the custom error\n",
    "    total_samples = len(gt)\n",
    "    if total_samples == 0:\n",
    "        raise ValueError(\"The length of ground truth cannot be zero.\")\n",
    "    \n",
    "    error = np.sum(weighted_cm) / total_samples\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Separate features and target\n",
    "X_train = training_data.drop(columns=['Class', 'Perform'])\n",
    "y_train = training_data['Class']\n",
    "\n",
    "# Step 6: Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(test_data)\n",
    "\n",
    "# Convert scaled data back to DataFrame for easier manipulation\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_train.columns)\n",
    "\n",
    "# Step 7: Perform Exhaustive Feature Selection\n",
    "# Split the data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_scaled, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "# model = LogisticRegression(max_iter=1000)\n",
    "# model = SVC(kernel='linear')\n",
    "model = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on selected features\n",
    "model.fit(X_train_split, y_train_split)\n",
    "y_val_pred = model.predict(X_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 0.93625\n",
      "Accuracy: 0.455\n",
      "Precision: 0.38341472672064775\n",
      "Recall: 0.455\n",
      "F1 Score: 0.41483851115720377\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.41      0.42      0.41       297\n",
      "           0       0.00      0.00      0.00       121\n",
      "           1       0.49      0.63      0.55       382\n",
      "\n",
      "    accuracy                           0.46       800\n",
      "   macro avg       0.30      0.35      0.32       800\n",
      "weighted avg       0.38      0.46      0.41       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_val_split, y_val_pred)\n",
    "precision = precision_score(y_val_split, y_val_pred, average='weighted')\n",
    "recall = recall_score(y_val_split, y_val_pred, average='weighted')\n",
    "f1 = f1_score(y_val_split, y_val_pred, average='weighted')\n",
    "report = classification_report(y_val_split, y_val_pred)\n",
    "\n",
    "print(f\"Error: {calculate_custom_error(y_val_pred, y_val_split)}\")\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test_scaled)\n",
    "np.savetxt('recursive_search.txt', y_test_pred.astype(int), fmt='%d', newline='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import numpy as np\n",
    "import joblib\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "# Step 1: Load Data\n",
    "training_data = pd.read_csv('../data/training_data.csv', delimiter=';')\n",
    "test_data = pd.read_csv('../data/test_data_no_target.csv', delimiter=';')\n",
    "\n",
    "# Step 2: Convert Numerical Columns from strings to floats\n",
    "def convert_to_float(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' and col not in ['Group', 'Class', 'Perform']:\n",
    "            df[col] = df[col].str.replace(',', '.').astype(float)\n",
    "    return df\n",
    "\n",
    "# Apply conversion to the training data\n",
    "training_data = convert_to_float(training_data)\n",
    "test_data = convert_to_float(test_data)\n",
    "\n",
    "# Step 3: One-Hot Encoding for the 'Group' column\n",
    "training_data = pd.get_dummies(training_data, columns=['Group'])\n",
    "test_data = pd.get_dummies(test_data, columns=['Group'])\n",
    "\n",
    "# Ensure the test set has the same columns as the training set\n",
    "missing_cols = set(training_data.columns) - set(test_data.columns) - {'Class', 'Perform'}\n",
    "for col in missing_cols:\n",
    "    test_data[col] = 0\n",
    "test_data = test_data[training_data.columns.drop(['Class', 'Perform'])]\n",
    "\n",
    "# Step 4: Separate features and target\n",
    "X_train = training_data.drop(columns=['Class', 'Perform'])\n",
    "y_train = training_data['Class']\n",
    "\n",
    "# Step 5: Handle Missing Values with KNN Imputer\n",
    "# Apply ColumnTransformer to apply KNNImputer to numerical columns only\n",
    "numerical_cols = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_cols = X_train.select_dtypes(include=['uint8']).columns.tolist()  # One-hot encoded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', KNNImputer(n_neighbors=5), numerical_cols),\n",
    "        ('cat', 'passthrough', categorical_cols)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Step 6: Scale the features\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('imputer', preprocessor),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "X_train_processed = pipeline.fit_transform(X_train)\n",
    "X_test_processed = pipeline.transform(test_data)\n",
    "\n",
    "# Convert processed data back to DataFrame for easier manipulation\n",
    "X_train_processed = pd.DataFrame(X_train_processed, columns=numerical_cols + categorical_cols)\n",
    "X_test_processed = pd.DataFrame(X_test_processed, columns=numerical_cols + categorical_cols)\n",
    "\n",
    "# Step 7: Perform Sequential Feature Selection\n",
    "# Split the data into training and validation sets\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train_processed, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "STOPPING EARLY DUE TO KEYBOARD INTERRUPT..."
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m sfs \u001b[38;5;241m=\u001b[39m sfs\u001b[38;5;241m.\u001b[39mfit(X_train_split, y_train_split)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Print the selected features and their performance\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_feature_names_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelected features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mselected_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Perform SFS\n",
    "sfs = SFS(model, \n",
    "        k_features='best', \n",
    "        forward=True, \n",
    "        floating=False, \n",
    "        scoring='accuracy', \n",
    "        cv=5, \n",
    "        n_jobs=-1, \n",
    "        verbose=2)\n",
    "sfs = sfs.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Print the selected features and their performance\n",
    "selected_features = list(sfs.k_feature_names_)\n",
    "print(f\"Selected features: {selected_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
